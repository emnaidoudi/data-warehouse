{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import to_json\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrameWriter\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"mongodb://accretioadmin:adminaccretio&2017@172.17.0.1:27017\"\n",
    "conf = SparkConf()\\\n",
    "    .setAppName(\"pyspark mongo\")\\\n",
    "    .setMaster(\"local[*]\")\\\n",
    "    .set(\"spark.mongodb.input.uri\", \"mongodb://accretioadmin:adminaccretio&2017@accretio-2-tnr.advyteam.com:27017\")\\\n",
    "    .set(\"spark.mongodb.input.database\", \"coreRh\")\\\n",
    "    .set(\"spark.mongodb.input.collection\", \"employee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sc' in locals():\n",
    "    sc.stop()\n",
    "sc = SparkContext(conf=conf)\n",
    "sql_context = SQLContext(sc)\n",
    "df = sql_context\\\n",
    "    .read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .load()\n",
    "    #.schema(schema)\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "#     StructField(\"StatDate\", TimestampType()),\n",
    "#     StructField(\"EndDate\", TimestampType()),\n",
    "    StructField(\"CRCivility\", StringType()),\n",
    "    StructField(\"Status\", StringType()),\n",
    "    StructField(\"FirstHiringDate\", TimestampType()),\n",
    "    StructField(\"LastHiringDate\", TimestampType()),\n",
    "    StructField(\"EmployeeAlternateKey\", StringType()),\n",
    "    StructField(\"FirstName\", StringType()),\n",
    "    StructField(\"LastName\", StringType()),\n",
    "    StructField(\"BirthDate\", TimestampType()),\n",
    "    StructField(\"CRBirthCountry\", StringType()),\n",
    "    StructField(\"CRGender\", StringType()),\n",
    "    StructField(\"CRNationality\", StringType()),\n",
    "    StructField(\"PhoneNumber\", StringType()),\n",
    "    StructField(\"Email\", StringType()),\n",
    "    StructField(\"CRMaritalSituation\", StringType()),\n",
    "    StructField(\"CRLeaveReason\", StringType()),\n",
    "    StructField(\"CREmployeeStatus\", StringType()),\n",
    "    StructField(\"CREmployeeType\", StringType()),\n",
    "    StructField(\"FamilyMembersInCharge\", IntegerType()),\n",
    "    StructField(\"HiringCompany\", StringType()),\n",
    "    StructField(\"CRHiringReason\", StringType()),\n",
    "    StructField(\"CRContractType\", StringType()),\n",
    "    StructField(\"AffectationDate\", TimestampType()),\n",
    "    StructField(\"LeaveDate\", TimestampType()), \n",
    "    \n",
    "    StructField(\"Address\", StringType()),   \n",
    "    \n",
    "#     StructField(\"AffectedOE\", TimestampType()),\n",
    "#     StructField(\"AffectedJob\", TimestampType()),\n",
    "#     StructField(\"AffectedPosition\", TimestampType()),\n",
    "#     StructField(\"AffectedSite\", TimestampType()),\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_fields = dict()\n",
    "crs = [\"CRCivility\",\"CRSexe\",\"CRNationality\",\"CRMaritalSituation\",\"CRStatus\",\"CREmployeeType\",\"CRHiringReason\",\"CRContractType\"]\n",
    "sql_field=[\"Civility\",\"Gender\",\"Nationality\",\"MaritalSituation\",\"EmployeeStatus\",\"EmployeeType\",\"HiringReason\",\"ContractType\"]\n",
    "#CRCountry : BirthCountry    CRLeaveReason  : LeaveReason\n",
    "\n",
    "crs_fields = dict(zip(crs, sql_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(obj, att):\n",
    "    if hasattr(obj, att) and obj is not None:\n",
    "          return obj[att] \n",
    "    else:\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def familyMembersInCharge(obj):\n",
    "    att = 'family_members'\n",
    "    if hasattr(obj, att):\n",
    "        if (obj[att] is not None):\n",
    "            return len(obj[att])\n",
    "        else :\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_att(obj,att):\n",
    "    t=\"\"\n",
    "    if verify(obj,att) is not None:\n",
    "        for j in obj[att]:\n",
    "            t=t+\",\"+j if len(t)>0 else j\n",
    "        return t\n",
    "    else:\n",
    "        return \"\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_document(obj,att,att2):\n",
    "    t=\"\"\n",
    "    if verify(obj,att) is not None:\n",
    "        for j in obj[att]:\n",
    "            if verify(j,att2) is not None:\n",
    "                #print(\"in\"+j[att2])\n",
    "                t=t+\",\"+j[att2] if len(t)>0 else j[att2]\n",
    "        return t\n",
    "    else:\n",
    "        return \"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def nationalities_list(obj):\n",
    "#     t=\"\"\n",
    "#     att = 'nationalities'\n",
    "#     if obj[att] is not None:\n",
    "#         for j in obj[att]:\n",
    "#             t=t+\",\"+j if len(t)>0 else j\n",
    "#         return t\n",
    "#     else:\n",
    "#         return \"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(verify(df.collect()[1],'family_members'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'current_timestamp()[value]'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_timestamp().value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affectation(k):\n",
    "    m=dict()\n",
    "    t=\"\"\n",
    "    m[\"StartDate\"]=  verify(k,\"birth_day\")  #datetime.datetime.now()\n",
    "    m[\"EndDate\"]=   verify(k,\"birth_day\")#current_timestamp()\n",
    "    m['CRCivility']=verify(k,\"civility\")\n",
    "    m['Status']=\"Actif\"\n",
    "    m['FirstHiringDate']=verify(k,\"first_hiring_date\")\n",
    "    m['LastHiringDate']=verify(k,\"last_hiring_date\")\n",
    "    m['EmployeeAlternateKey']=verify(k,\"registration_number\")\n",
    "    m['FirstName']=verify(k,\"first_name\")\n",
    "    m['LastName']=verify(k,\"last_name\")\n",
    "    m['BirthDate']=verify(k,\"birth_day\")\n",
    "    m['CRBirthCountry']=verify(k,\"birth_country\")\n",
    "    m['CRGender']=verify(k,\"sexe\")\n",
    "    m['CRNationality']=list_att(k,'nationalities')\n",
    "    m['Address']=list_of_document(k,'addresses','address')\n",
    "    m['PhoneNumber']=verify(k,\"phone\")\n",
    "    m['Email']=verify(k,\"email\")\n",
    "    \n",
    "    ms=verify(k,\"marital_situation\")\n",
    "    if ms is not None :\n",
    "        m['CRMaritalSituation']=verify(ms,\"marital_situation\")\n",
    "    else :\n",
    "        m['CRMaritalSituation'] = None  \n",
    "        \n",
    "    lr=verify(k,\"hiring_exit_history\")\n",
    "    if lr is not None :\n",
    "        m['CRLeaveReason']=verify(lr,\"leave_reason\")\n",
    "    else :\n",
    "        m['CRLeaveReason'] = \"\"\n",
    "    \n",
    "    es=verify(k,\"status\")\n",
    "    if es is not None :\n",
    "        m['CREmployeeStatus']=verify(es,\"effective_status\")\n",
    "    else :\n",
    "        m['CREmployeeStatus'] = None   \n",
    "        \n",
    "    et=verify(k,\"employee_type\")\n",
    "    if et is not None :\n",
    "        m['CREmployeeType']=verify(et,\"employee_type\")\n",
    "    else :\n",
    "        m['CREmployeeType'] = None  \n",
    "        \n",
    "    m['FamilyMembersInCharge']=familyMembersInCharge(k)\n",
    "    \n",
    "    hc=verify(k,\"hiring_exit\")\n",
    "    if hc is not None :\n",
    "        x= verify(hc,\"hiring_company\")\n",
    "        m['HiringCompany']=x[1].oid if x is not None else None \n",
    "    else :\n",
    "        m['HiringCompany'] = None    \n",
    "    \n",
    "    hr=verify(k,\"hiring_exit\")\n",
    "    if hr is not None :\n",
    "        m['CRHiringReason']=verify(hr,\"hiring_reason\")\n",
    "    else :\n",
    "        m['CRHiringReason'] = None \n",
    "        \n",
    "\n",
    "    ct=verify(k,\"contract\")\n",
    "    if ct is not None :\n",
    "        m['CRContractType']=verify(ct,\"contract_type\")\n",
    "    else :\n",
    "        m['CRContractType'] = None \n",
    "        \n",
    "    ad=verify(k,\"affectation\")\n",
    "    if ad is not None :\n",
    "        m['AffectationDate']=verify(ad,\"affectation_effective_date\")\n",
    "    else :\n",
    "        m['AffectationDate'] = None #datetime.datetime.time   \n",
    "        \n",
    "    ld=verify(k,\"hiring_exit\")\n",
    "    if ld is not None :\n",
    "        m['LeaveDate']=  verify(ld,\"leave_date\")   #verify(ld,\"leave_date\")\n",
    "    else :\n",
    "        m['LeaveDate'] =  None #datetime.datetime.time         \n",
    "        \n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# listt=[]\n",
    "listt=[]\n",
    "listk=[]\n",
    "for k in df.collect():\n",
    "    t=\"\"\n",
    "    temp = affectation(k)\n",
    "    \n",
    "    aoe=verify(k,\"affectation\")\n",
    "    if aoe is not None :\n",
    "        x=verify(aoe,\"affected_organizational_entity\")\n",
    "        temp['AffectedOE']=x[1].oid if x is not None else None\n",
    "    else :\n",
    "        temp['AffectedOE'] = None\n",
    "        \n",
    "    \n",
    "    aj=verify(k,\"affectation\")\n",
    "    if aj is not None :\n",
    "        x=verify(aj,\"affected_job\")\n",
    "        if x is not None:\n",
    "            temp['AffectedJob']=x[1].oid\n",
    "        else:\n",
    "            temp['AffectedJob'] = None\n",
    "    else :\n",
    "        temp['AffectedJob'] = None\n",
    "        \n",
    "    ap=verify(k,\"affectation\")\n",
    "    if ap is not None :\n",
    "        x=verify(ap,\"affected_position\")\n",
    "        temp['AffectedPosition']=x[1].oid if x is not None else None\n",
    "    else :\n",
    "        temp['AffectedPosition'] = None         \n",
    "\n",
    "    asi=verify(k,\"hiring_exit\")\n",
    "    if asi is not None :\n",
    "        x=verify(asi,\"affected_site\")\n",
    "        if x is not None:\n",
    "            temp['AffectedSite']=x[1].oid\n",
    "        else:\n",
    "            temp['AffectedSite'] = None\n",
    "    else :\n",
    "        temp['AffectedSite'] = None\n",
    "       \n",
    "        \n",
    "    output = Row(**temp)\n",
    "    listk.append(output)\n",
    "\n",
    "listt.extend(listk)\n",
    "\n",
    "rdd = sc.parallelize(listt)\n",
    "df1=sql_context.createDataFrame(rdd,samplingRatio=0.2)# ,schema=schema\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.format('com.databricks.spark.csv').save('csv_files/employee.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'sc' in locals():\n",
    "#     sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cr(cr_name, field_name):  # field_name is the field name in the sql db\n",
    "    global sc\n",
    "    sc.stop()\n",
    "    conf = SparkConf()\\\n",
    "    .setAppName(\"pyspark mongo\")\\\n",
    "    .setMaster(\"local[*]\")\\\n",
    "    .set(\"spark.driver.allowmultiplecontexts\",True)\\\n",
    "    .set(\"spark.mongodb.input.uri\", \"mongodb://accretioadmin:adminaccretio&2017@accretio-2-tnr.advyteam.com:27017\")\\\n",
    "    .set(\"spark.mongodb.input.database\", \"dataAndTranslation\")\\\n",
    "    .set(\"spark.mongodb.input.collection\", cr_name)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql_context = SQLContext(sc)\n",
    "    df = sql_context\\\n",
    "    .read\\\n",
    "    .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .load()\n",
    "    \n",
    "    listt=[]\n",
    "    listk=[]\n",
    "\n",
    "    if(cr_name == \"CRCountry\")  :\n",
    "        for i in df.collect():\n",
    "            temp={}\n",
    "            temp['code']=i.code\n",
    "            temp[f'{field_name}']=i.label[0].value\n",
    "            output = Row(**temp)\n",
    "            listt.append(output)\n",
    "            \n",
    "    else:    \n",
    "        for i in df.collect():\n",
    "            temp={}\n",
    "            #print(i.value.label[0][0])\n",
    "            temp['code']=i.code\n",
    "            #print(\"here\",i.value.label[0].value)\n",
    "            temp[f'{field_name}']=i.value.label[0][0] # HERE WE PUT THE NAÙE OF THE SQL FIELD\n",
    "            output = Row(**temp)\n",
    "            listt.append(output)\n",
    "    rdd = sc.parallelize(listt)\n",
    "    c=sql_context.createDataFrame(rdd)    \n",
    "    c.write.format('com.databricks.spark.csv').save(f'csv_files/{cr_name}.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_fields = dict()\n",
    "crs = [\"CRCivility\",\"CRSexe\",\"CRNationality\",\"CRMaritalSituation\",\"CRStatus\",\"CREmployeeType\",\"CRHiringReason\",\"CRContractType\",\"CRCountry\",\"CRLeaveReason\"]\n",
    "sql_field=[\"Civility\",\"Gender\",\"Nationality\",\"MaritalSituation\",\"EmployeeStatus\",\"EmployeeType\",\"HiringReason\",\"ContractType\",\"BirthCountry\",\"LeaveReason\"]\n",
    "#   CRLeaveReason :LeaveReason\n",
    "\n",
    "crs_fields = dict(zip(crs, sql_field))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in crs_fields.items():\n",
    "    load_cr(key,value)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sc' in locals():\n",
    "    sc.stop()\n",
    "conf=SparkConf()\n",
    "sc=SparkContext(conf=conf)\n",
    "sql_context=SQLContext(sc)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee=sql_context.read.csv('csv_files/employee.csv',header='true') #,schema=schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_crs = list()\n",
    "for key in crs_fields:\n",
    "    csv_cr = sql_context.read.csv(f'csv_files/{key}.csv',header='true')\n",
    "    list_crs.append(csv_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_crs[0].code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = employee.join(list_crs[0],employee[\"CRCivility\"]==list_crs[0].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.join(list_crs[3],employee[\"CRMaritalSituation\"]==list_crs[3].code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = employee\n",
    "for d in list_crs:\n",
    "    #print(\"1\"+d.code)\n",
    "    df = df.join(d, df[\"CR\"+d.columns[0]]==d.code, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = lambda col: 'CR' in col or col==\"code\"\n",
    "new_df = df.drop(*filter(condition, df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns_to_drop = crs+[\"code\"]\n",
    "# df_new = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=new_df.where(\"EmployeeAlternateKey!='None'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.write.format('com.databricks.spark.csv').save(f'csv_files/final_employee.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.DataFrame(new_df.head(3), columns=new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#,\"ContractType\",\"Nationality\",\"Address\",\"EmployeeStatus\",\"BirthCountry\",\"LeaveReason\",\"LastHiringDate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas[[\"EmployeeAlternateKey\",\"FirstName\",\"Address\",\"FirstHiringDate\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=sql_context.createDataFrame(new_df.rdd, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.printSchema()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=new_df.select(\"FirstName\",to_timestamp(new_df.FirstHiringDate).alias('FirstHiringDate'),\"Address\" , to_timestamp(new_df.LastHiringDate).alias('LastHiringDate'), to_timestamp(new_df.BirthDate).alias('BirthDate') ,  to_timestamp(new_df.AffectationDate).alias('AffectationDate'), to_timestamp(new_df.LeaveDate).alias('LeaveDate'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df = new_df.withColumn(\"FamilyMembersInCharge\", new_df[\"FamilyMembersInCharge\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dates=[\"FirstHiringDate\",\"LastHiringDate\",\"BirthDate\",\"AffectationDate\",\"LeaveDate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals():\n",
    "    del df\n",
    "for i in list_dates:\n",
    "    if 'df' in locals():\n",
    "        new_df=df\n",
    "    df= new_df.withColumn(i, new_df[i].cast(TimestampType()))\n",
    "df = df.withColumn(\"FamilyMembersInCharge\", new_df[\"FamilyMembersInCharge\"].cast(IntegerType()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = new_df.withColumn(\"FirstHiringDate\", new_df[\"FirstHiringDate\"].cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sc' in locals():\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = [\"organizational_entity\",\"company\",\"site\",\"position\",\"job\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_collection(collection_name):\n",
    "    global sc\n",
    "    sc.stop()\n",
    "    conf = SparkConf()\\\n",
    "        .setAppName(\"pyspark mongo\")\\\n",
    "        .setMaster(\"local[*]\")\\\n",
    "        .set(\"spark.mongodb.input.uri\", \"mongodb://accretioadmin:adminaccretio&2017@accretio-2-tnr.advyteam.com:27017\")\\\n",
    "        .set(\"spark.mongodb.input.database\", \"coreRh\")\\\n",
    "        .set(\"spark.mongodb.input.collection\", collection_name)\n",
    "    sc = SparkContext(conf=conf)\n",
    "    sql_context = SQLContext(sc)\n",
    "    df = sql_context\\\n",
    "        .read\\\n",
    "        .format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .load()\n",
    "    \n",
    "    df=df.withColumn('_id', df._id.oid)\n",
    "    df=df.selectExpr('_id','label')\n",
    "    \n",
    "    \n",
    "    df.write.format('com.databricks.spark.csv').save(f'csv_files/ref/{collection_name}.csv',header=True)\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in collections:    \n",
    "    load_collection(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sc' in locals():\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf=SparkConf()\n",
    "sc=SparkContext(conf=conf)\n",
    "sql_context=SQLContext(sc)\n",
    "\n",
    "employee=sql_context.read.csv('csv_files/final_employee.csv',header='true')\n",
    "\n",
    "# list_ref=list()\n",
    "# for i in collections:\n",
    "#     list_ref.append(sql_context.read.csv(f'csv_files/ref/{i}.csv',header='true'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions=sql_context.read.csv('csv_files/ref/position.csv',header='true')\n",
    "site=sql_context.read.csv('csv_files/ref/site.csv',header='true')\n",
    "job=sql_context.read.csv('csv_files/ref/job.csv',header='true')\n",
    "organization=sql_context.read.csv('csv_files/ref/organizational_entity.csv',header='true')\n",
    "company=sql_context.read.csv('csv_files/ref/company.csv',header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "employee.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=employee.join(organization,employee.AffectedOE==organization._id,how='left')\n",
    "df = df.drop(\"AffectedOE\",\"_id\")\n",
    "df = df.withColumnRenamed(\"label\", \"AffectedOE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = empor.selectExpr(\"label as AffectedOE\")\n",
    "#list_cols = ','.join(empor.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.join(positions,employee.AffectedPosition==positions._id,how='left')\n",
    "df = df.drop(\"AffectedPosition\",\"_id\")\n",
    "df = df.withColumnRenamed(\"label\", \"AffectedPosition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.join(site,employee.AffectedSite==site._id,how='left')\n",
    "df = df.drop(\"AffectedSite\",\"_id\")\n",
    "df = df.withColumnRenamed(\"label\", \"AffectedSite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.join(company,employee.HiringCompany==company._id,how='left')\n",
    "df = df.drop(\"HiringCompany\",\"_id\")\n",
    "df = df.withColumnRenamed(\"label\", \"HiringCompany\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.join(job,employee.AffectedJob==job._id,how='left')\n",
    "df = df.drop(\"AffectedJob\",\"_id\")\n",
    "df = df.withColumnRenamed(\"label\", \"AffectedJob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = pd.DataFrame(df.head(3), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas[[\"AffectedSite\",\"HiringCompany\",\"AffectedPosition\",\"AffectedOE\",\"AffectedJob\",\"StatDate\",\"Nationality\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pandas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to postgres sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql.Driver:postgresql-42.2.12 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrameWriter\n",
    "\n",
    "def write_df_to_table(df_writer, table):\n",
    "    jdbc_url = \"jdbc:postgresql://172.17.0.1:5432/AccretioDW\"\n",
    "    mode = \"append\"\n",
    "    properties = {\"user\": \"postgres\",\n",
    "                  \"password\": \"123456789\",\n",
    "                  \"driver\": \"org.postgresql.Driver\"}\n",
    "\n",
    "    print(\"Writing to {}\".format(table))\n",
    "    df_writer.jdbc(jdbc_url, table, mode, properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write_df_to_table(DataFrameWriter(df),'DimEmployee')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
